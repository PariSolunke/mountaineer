{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pari/miniconda3/envs/mountaineer/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/pari/miniconda3/envs/mountaineer/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/pari/miniconda3/envs/mountaineer/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "/home/pari/miniconda3/envs/mountaineer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/pari/miniconda3/envs/mountaineer/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "from mountaineer import Mountaineer\n",
    "from gale import create_mapper, bootstrap_mapper_params, bottleneck_distance\n",
    "from gale.plots import plot_mapper, plot_ext_persistance_diagram\n",
    "from gudhi.cover_complex import MapperComplex\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data found. Loading...done.\n",
      "\n",
      "Number of samples: Train 111509 | Test 47790\n",
      "Number of features: 12\n",
      "Train - Class 0: 48081 | Class 1: 63428\n",
      "Test - Class 0: 20455 | Class 1: 27335\n"
     ]
    }
   ],
   "source": [
    "data_name = 'ACSEmployment'\n",
    "data_path = f'dataset/{data_name}/'\n",
    "\n",
    "if Path(data_path+'X_train.csv').is_file():\n",
    "    print('Processed data found. Loading...', end='')\n",
    "    X_train = pd.read_csv(data_path+'X_train.csv')\n",
    "    X_test = pd.read_csv(data_path+'X_test.csv')\n",
    "    y_train = pd.read_csv(data_path+'y_train.csv')\n",
    "    y_test = pd.read_csv(data_path+'y_test.csv')\n",
    "    print('done.\\n')\n",
    "\n",
    "else:\n",
    "    print('Processed data not found.')\n",
    "\n",
    "    from folktables import ACSDataSource, ACSEmployment\n",
    "\n",
    "    print('Downloading data...', end='')\n",
    "    data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "    acs_data = data_source.get_data(states=['NY'])\n",
    "    variables, target, group = ACSEmployment.df_to_pandas(acs_data)\n",
    "    target = target['ESR']\n",
    "    print('done.')\n",
    "\n",
    "    print('Processing data...', end='')\n",
    "    # Remove people under 18\n",
    "    age_mask = variables['AGEP'] >= 18\n",
    "    variables = variables[age_mask]\n",
    "    target = target[age_mask].astype(int)\n",
    "    # Map binary features\n",
    "    bin_columns = ['SEX', 'DIS', 'DEAR', 'DEYE', 'DREM']\n",
    "    variables['MALE'] = variables['SEX'].map({1: 1, 2: 0})\n",
    "    # variables['DISABILITY'] = variables['DIS'].map({1: 1, 2: 0}) # Remove disability\n",
    "    variables['HEARING_DIFFICULTY'] = variables['DEAR'].map({1: 1, 2: 0})\n",
    "    variables['VISION_DIFICULTY'] = variables['DEYE'].map({1: 1, 2: 0})\n",
    "    variables['COGNITIVE_DIFFICULTY'] = variables['DREM'].map({1: 1, 2: 0})\n",
    "    variables.drop(columns=bin_columns, inplace=True)\n",
    "    # Map categorical features\n",
    "    cat_columns = ['MIL', 'MIG', 'MAR', 'RAC1P', 'CIT']\n",
    "    variables['BEEN_MILITARY'] = variables['MIL'].map({1: 1, 2: 1, 3: 1, 4: 0})\n",
    "    variables['MOBILITY'] = variables['MIG'].map({1: 0, 2: 1, 3: 1})\n",
    "    variables['MARRIED'] = variables['MAR'].map({1: 1, 2: 0, 3: 0, 4: 0, 5: 0})\n",
    "    variables['WHITE'] = variables['RAC1P'].map({1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0})\n",
    "    #variables['HIGH_SCHOOL'] = (variables['SCHL'] > 15).astype(int)\n",
    "    #variables['COLLEGE'] = (variables['SCHL'] > 17).astype(int)\n",
    "    #variables['GRAD_SCHOOL'] = (variables['SCHL'] > 21).astype(int)\n",
    "    variables['NATURALIZED'] = (variables['CIT'] == 4).astype(int)\n",
    "    variables['NOT_US_CITIZEN'] = (variables['CIT'] == 5).astype(int)\n",
    "    variables.drop(columns=cat_columns, inplace=True)\n",
    "    \n",
    "    # Features to drop\n",
    "    drop_columns = ['ESP', 'RELP', 'ANC', 'NATIVITY']\n",
    "    variables.drop(columns=drop_columns, inplace=True)\n",
    "    print('done.')\n",
    "\n",
    "    print('Splitting and saving train and test data...', end='')\n",
    "    # Splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(variables, target, test_size=0.3, random_state=42)\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[['AGEP', 'SCHL']])\n",
    "    X_train[['AGEP', 'SCHL']] = scaler.transform(X_train[['AGEP', 'SCHL']])\n",
    "    X_test[['AGEP', 'SCHL']] = scaler.transform(X_test[['AGEP', 'SCHL']])\n",
    "    # Saving\n",
    "    X_train.to_csv(data_path+'X_train.csv', index=False)\n",
    "    X_test.to_csv(data_path+'X_test.csv', index=False)\n",
    "    y_train.to_csv(data_path+'y_train.csv', index=False)\n",
    "    y_test.to_csv(data_path+'y_test.csv', index=False)\n",
    "    print('done.\\n')\n",
    "\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()\n",
    "\n",
    "print(f\"Number of samples: Train {X_train.shape[0]} | Test {X_test.shape[0]}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")\n",
    "print(f\"Train - Class 0: {y_train[y_train==0].shape[0]} | Class 1: {y_train[y_train==1].shape[0]}\")\n",
    "print(f\"Test - Class 0: {y_test[y_test==0].shape[0]} | Class 1: {y_test[y_test==1].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"RBF SVM\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"XGBoost\"\n",
    "]\n",
    "\n",
    "# If the predictions and explanations exist, we don't need the models\n",
    "predictions_path = data_path+'skmodels_pred.p'\n",
    "lime_exp_path = data_path+'lime_exp_all_feat.p'\n",
    "if not Path(predictions_path).is_file() or not Path(lime_exp_path).is_file():\n",
    "    \n",
    "    skmodels_path = data_path+'skmodels.p'\n",
    "\n",
    "    if Path(skmodels_path).is_file():\n",
    "        print('Models found. Loading...')\n",
    "        classifiers = pickle.load(open(skmodels_path, 'rb'))\n",
    "        scores = pickle.load(open(data_path+'model_scores.p', 'rb'))\n",
    "        for name, clf in zip(names, classifiers):\n",
    "            print(f\"{name} - Accuracy: {scores[name]:.4f}\")\n",
    "        print('Done.\\n')\n",
    "\n",
    "    else:\n",
    "        print('Models not found.')\n",
    "        classifiers = [\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            SVC(kernel='rbf', gamma='scale', C=1, probability=True),\n",
    "            CalibratedClassifierCV(RandomForestClassifier(n_estimators=100, min_samples_split=10), method='isotonic'),\n",
    "            MLPClassifier(alpha=0.01, max_iter=1000, learning_rate='adaptive', learning_rate_init=0.005,\n",
    "                                    early_stopping=True, hidden_layer_sizes=(128,)),\n",
    "            CalibratedClassifierCV(AdaBoostClassifier(n_estimators=500), method='isotonic'),\n",
    "            CalibratedClassifierCV(XGBClassifier(), method='isotonic')\n",
    "        ]\n",
    "        scores = {}\n",
    "\n",
    "        for name, clf in zip(names, classifiers):\n",
    "            print(f\"Training {name}...\", end=' ')\n",
    "            clf.fit(X_train_np, y_train)\n",
    "            score = clf.score(X_test_np, y_test)\n",
    "            scores[name] = score\n",
    "            print(f\"done. Accuracy: {score:.4f}\")\n",
    "\n",
    "        print('Saving models...', end='')\n",
    "        pickle.dump(classifiers, open(skmodels_path, 'wb'))\n",
    "        pickle.dump(scores, open(data_path+'model_scores.p', 'wb'))\n",
    "        print('done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions found. Loading...done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_path = data_path+'skmodels_pred.p'\n",
    "\n",
    "if Path(predictions_path).is_file():\n",
    "    print('Predictions found. Loading...', end='')\n",
    "    predictions = pickle.load(open(predictions_path, 'rb'))\n",
    "    print('done.\\n')\n",
    "\n",
    "else:\n",
    "    print('Predictions not found.')\n",
    "    predictions = {}\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        print(f\"Generating predictions for {name}...\", end=' ')\n",
    "        predictions[name] = clf.predict_proba(X_test_np)[:, 1]\n",
    "        print(f\"done.\")\n",
    "    \n",
    "    print('Saving predictions...', end='')\n",
    "    pickle.dump(predictions, open(predictions_path, 'wb'))\n",
    "    print('done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lime(X, model, num_features=5, num_samples=1000):\n",
    "    explainer = LimeTabularExplainer(X, \n",
    "                                    #categorical_features=list(range(1,len(X.columns))),\n",
    "                                    discretize_continuous=False, \n",
    "                                    sample_around_instance=True,\n",
    "                                    random_state=2023)\n",
    "    lime_exp = []\n",
    "    for x in X:\n",
    "        exp = explainer.explain_instance(x, model.predict_proba, \n",
    "                                        num_features=num_features, \n",
    "                                        num_samples=num_samples)\n",
    "        tmp = [0 for i in range(X.shape[1])]\n",
    "\n",
    "        for e in exp.as_list():\n",
    "            tmp[int(e[0])] = e[1]\n",
    "        lime_exp.append(tmp)\n",
    "    lime_exp = np.array(lime_exp)\n",
    "    return lime_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME explanations found. Loading...done.\n",
      "\n",
      "Getting Logistic Regression \"explanations\"... done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lime_exp_path = data_path+'lime_exp_all_feat.p'\n",
    "\n",
    "if Path(lime_exp_path).is_file():\n",
    "    print('LIME explanations found. Loading...', end='')\n",
    "    lime_exp = pickle.load(open(lime_exp_path, 'rb'))\n",
    "    print('done.\\n')\n",
    "\n",
    "else:\n",
    "    print('LIME explanations not found.')\n",
    "    lime_exp = {}\n",
    "    \n",
    "    for name, clf in zip(names, classifiers):\n",
    "        if name == \"Logistic Regression\":\n",
    "            continue\n",
    "        print(f\"Running LIME for {name}...\", end=' ')\n",
    "        lime_exp[name] = run_lime(X_test_np, clf, num_features=X_test.shape[1], num_samples=1000)\n",
    "        print(\"done.\")\n",
    "\n",
    "    print('Saving LIME explanations...', end='')\n",
    "    pickle.dump(lime_exp, open(lime_exp_path, 'wb'))\n",
    "    print('done.\\n')\n",
    "\n",
    "print('Getting Logistic Regression \"explanations\"...', end=' ')\n",
    "logreg_coef_path = data_path+'logreg_coef.p'\n",
    "if Path(logreg_coef_path).is_file():\n",
    "    logreg_expl = pickle.load(open(logreg_coef_path, 'rb'))\n",
    "else:\n",
    "    logreg_coef = classifiers[0].coef_\n",
    "    logreg_expl = np.zeros((X_test_np.shape[0], X_test_np.shape[1]))\n",
    "    for i in range(X_test_np.shape[0]):\n",
    "        logreg_expl[i,:] = logreg_coef\n",
    "    pickle.dump(logreg_expl, open(logreg_coef_path, 'wb'))\n",
    "print('done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = data_path+'deltas_lime_all_feat.p'\n",
    "\n",
    "if Path(delta_path).is_file():\n",
    "    deltas = pickle.load(open(delta_path, 'rb'))\n",
    "\n",
    "else:\n",
    "    deltas = {}\n",
    "\n",
    "    for name in names[1:]:\n",
    "        mapper = MapperComplex(input_type=\"point cloud\")\n",
    "        if name == \"Logistic Regression\":\n",
    "            deltas[name] = mapper.estimate_scale(logreg_expl, N=500, beta=0.001)\n",
    "        else:\n",
    "            deltas[name] = mapper.estimate_scale(lime_exp[name], N=500, beta=0.001)\n",
    "        print(f'Delta for {name}: {deltas[name]}')\n",
    "\n",
    "    pickle.dump(deltas, open(delta_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters found.\n",
      "Parameters for Logistic Regression: {'stability': 0, 'components': 1, 'resolution': 15, 'gain': 0.4, 'distance_threshold': 0.0001}\n",
      "Parameters for RBF SVM: {'stability': 0.04099066951530628, 'components': 12, 'resolution': 19, 'gain': 0.4, 'distance_threshold': 0.09353887333261668}\n",
      "Parameters for Random Forest: {'stability': 0.025183628892470472, 'components': 5, 'resolution': 16, 'gain': 0.4, 'distance_threshold': 0.05066481908093368}\n",
      "Parameters for Neural Net: {'stability': 0.03788933227874336, 'components': 6, 'resolution': 11, 'gain': 0.4, 'distance_threshold': 0.06967558165846405}\n",
      "Parameters for AdaBoost: {'stability': 0.025402341327028677, 'components': 6, 'resolution': 14, 'gain': 0.4, 'distance_threshold': 0.032939685843176296}\n",
      "Parameters for XGBoost: {'stability': 0.053781308651508125, 'components': 5, 'resolution': 11, 'gain': 0.4, 'distance_threshold': 0.050259411293833295}\n"
     ]
    }
   ],
   "source": [
    "params_path = data_path+'mapper_params_lime_all_feat_r10-20.p'\n",
    "\n",
    "gain = 0.4\n",
    "min_points_per_node = 0\n",
    "resolutions = [i for i in range(10, 21)]\n",
    "\n",
    "if Path(params_path).is_file():\n",
    "    print('Parameters found.')\n",
    "    mapper_params = pickle.load(open(params_path, 'rb'))\n",
    "    for name in names:\n",
    "        print(f'Parameters for {name}: {mapper_params[name]}')\n",
    "\n",
    "else:\n",
    "    mapper_params = {}\n",
    "\n",
    "    for name in names[1:]:\n",
    "        mapper_params[name] = bootstrap_mapper_params(lime_exp[name], predictions[name],\n",
    "                                                    resolutions=resolutions,\n",
    "                                                    gains=[gain],\n",
    "                                                    distances=[deltas[name]],\n",
    "                                                    min_points_per_node=[min_points_per_node],\n",
    "                                                    n=100, seed=2023,\n",
    "                                                    n_jobs=-1)\n",
    "        print(f'Parameters for {name}: {mapper_params[name]}')\n",
    "\n",
    "    mapper_params['Logistic Regression'] = {'stability': 0, \n",
    "                                            'components': 1,\n",
    "                                            'resolution': 15,\n",
    "                                            'gain': 0.4,\n",
    "                                            'distance_threshold': 0.0001\n",
    "                                            }\n",
    "\n",
    "    pickle.dump(mapper_params, open(params_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapper for Logistic Regression created.\n",
      "Mapper for RBF SVM created.\n",
      "Mapper for Random Forest created.\n",
      "Mapper for Neural Net created.\n",
      "Mapper for AdaBoost created.\n",
      "Mapper for XGBoost created.\n"
     ]
    }
   ],
   "source": [
    "mappers = {}\n",
    "\n",
    "mappers['Logistic Regression'] = create_mapper(logreg_expl, predictions['Logistic Regression'],\n",
    "                                                resolution=mapper_params['Logistic Regression']['resolution'], \n",
    "                                                gain=mapper_params['Logistic Regression']['gain'], \n",
    "                                                dist_thresh=mapper_params['Logistic Regression']['distance_threshold'],\n",
    "                                                min_points_per_node=min_points_per_node)\n",
    "print(f'Mapper for Logistic Regression created.')\n",
    "\n",
    "for name in names[1:]:\n",
    "    mappers[name] = create_mapper(lime_exp[name], predictions[name],\n",
    "                                  resolution=mapper_params[name]['resolution'], \n",
    "                                  gain=mapper_params[name]['gain'], \n",
    "                                  dist_thresh=mapper_params[name]['distance_threshold'],\n",
    "                                  min_points_per_node=min_points_per_node)\n",
    "    print(f'Mapper for {name} created.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of mapper outputs - minimum 2\n",
    "exp_list = list(mappers.keys())\n",
    "mapper_outputs=[mappers[mode] for mode in exp_list]\n",
    "predicted_prob = [predictions[mode] for mode in exp_list]\n",
    "explanation_vectors=[]\n",
    "explanation_vectors.append(logreg_expl)\n",
    "for exp in exp_list[1:]:\n",
    "    explanation_vectors.append(lime_exp[exp])\n",
    "\n",
    "explanation_list=[]\n",
    "for expl in explanation_vectors:\n",
    "    explanation_list.append(expl.tolist())\n",
    "\n",
    "expl_labels = exp_list\n",
    "class_labels = {1:'Employed', 0:\"Not employed\"}\n",
    "color_values = [predictions['Logistic Regression']]\n",
    "\n",
    "#column names of the dataframe\n",
    "column_names = np.array(X_train.columns)\n",
    "model_names = ['pred_LR', 'pred_SV', 'pred_RF', 'pred_NN', 'pred_AB', 'pred_XG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:1920px !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:1920px !important; }</style>\"))\n",
    "\n",
    "#visualize\n",
    "test = Mountaineer()\n",
    "\n",
    "test.visualize(X_test_np, y_test, predicted_prob, explanation_list, mapper_outputs, column_names, \n",
    "              expl_labels, class_labels, kamada_layout=False, model_comparison=True, model_names=model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
